## This section will provide information about the particular artcle

## Review Papers

[1. Explainable artificial intelligence: An analytical review ](https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1424)

### Aim
    To explore the field of Explainable Artificial Intelligence (XAI) and its significance in bridging 
    the gap between machine decisions and human understanding. 

### Main focuses
    Review of State-of-the-Art XAI Approaches Feature-Oriented and Global Explanations Concept Models and
    Human-Centric Explanations Critical Applications of XAI

### Methodology
    It involves a comprehensive review and analysis of various XAI approaches. Principles and techniques,
    strengths, limitations, and applicability of the approach

### Results
    Provides insights into various XAI methods, their underlying mechanisms, and their potential benefits 
    in different applications.

### Applications
    Medicine
    Natural Language Processing
    Criminal Justice
    Autonomous Driving

### Limitations
    Lack of Real-World Case Studies
    Does not provide a comprehensive comparison of these methods in terms of their effectiveness, applicability, and limitations.
    Not addressing Ethical and Societal Implications

[2. Explainable Artificial Intelligence: a Systematic Review](https://www.example.com](https://arxiv.org/pdf/2006.00093.pdf)https://arxiv.org/pdf/2006.00093.pdf)

### Aim
    To provide a comprehensive overview of the existing methods, theories, and evaluation techniques related to XAI and to define boundaries for the field.

### Main focuses
    Organizing Scattered Knowledge ,Hierarchical Classification System, Evolution and Diversity of Explanation Formats ,Unified Framework for XAI

### Methodology
    It involves a systematic literature review. The researchers began by searching Google Scholar using relevant keywords such as "explainable artificial 
    intelligence," "explainable machine learning," and "interpretable machine learning." After identifying relevant articles, they further explored their         
    bibliographies to uncover additional relevant studies. The review process led to the identification of about 350 research articles for analysis.

### Results
    Review Articles: These provide overviews of specific aspects of XAI.
    Theories and Notions: This cluster encompasses the concepts and theories related to explainability.
    Methods: This includes various techniques and approaches for generating explanations for AI models.
    Evaluation: This cluster covers methods and metrics used to evaluate the effectiveness of XAI methods.

### Applications
    Gives a guide for researchers interested in exploring the field of XAI. 
    Practitioners working with machine learning models can benefit from the categorized methods and techniques for generating explanations. 
    The discussion on legal requirements and regulations (such as GDPR) highlights the relevance of XAI in ensuring compliance with transparency 
    and explanation mandates.

### Limitations
    he XAI field is rapidly evolving, and the state of the art may have advanced since the time of the review. 
    The review might not fully capture insights from other fields (e.g., psychology, philosophy). 
    The article's methodology involves searching Google Scholar and examining bibliographies, which might result in a bias 
    towards more well-known or easily accessible research papers. 

    
[3. A historical perspective of explainable Artificial Intelligence](https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1391)

### Aim
    To provide an overview of XAI, and how it is understood in expert systems, machine learning, recommender systems, and neural-symbolic learning and reasoning 
    approaches.
    To provide the reader with a wide range of references, (s)he can use to gain a deeper understanding in the topic of XAI. 

### Main focuses
    Integration of Symbolic and Neural Approaches
    Explainability and Interpretability
    User-Centered Explanations
    Causal and Counterfactual Explanations

### Methodology
    Extracting decision trees, decision rules, and feature importance vectors to explain complex models.      
    Utilizing methods like LIME (Local Interpretable Model-agnostic Explanations) to generate explanations for specific instances     
    Ensuring that counterfactual explanations are truthful, human understandable, and applicable to GDPR requirements   

### Results
    Provides insights into the historical progression of AI systems and their explainability   
    Highlights the trade-offs between accuracy and transparency in explainable models 
    Emphasizes the need for user-centric explanations  

### Applications
    concepts discussed in the article have practical applications in a range of fields, including
    Healthcare
     Finance
    Legal 
    Autonomous Systems

### Limitations
     Discusses explanations in general terms but doesn't provide detailed insights into how different explanation methods are 
     suited to specific application contexts
     Lacks practical examples or case studies to demonstrate the real-world application


[4. Opportunities and Challenges in Explainable Artificial Intelligence](https://arxiv.org/pdf/2006.11371.pdf)

### Aim
    Providing a holistic view of thecurrent XAI landscape in deep learning and providing mathematical summaries of seminal work.

### Main focuses
    Discussing the different approaches and perspectives of researchers to address the problem of the explainability 
    of deep learning algorithms.
    
### Methodology
    Categorizing the XAI techniques based on their scope , methodology ,and explanation level or usage.
    Describing the main principles used in XAI research.
    Presenting the historical timeline for landmark studies in XAI.
    Then evaluating the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this 
    approach, and provide potential future directions to improve XAI evaluation.  

### Results
    Found out considerable research in XAI is focused in model-agnostic post-hoc explainability algorithms due to their easier 
    integration and wider reach and there is a large interest in additive and local surrogate models using superpixels of information 
    to evaluate the input feature attributions.

### Applications
    Mojorly on Mission Critical Applications.(Like Health Care, Self-Driving Vehicles,Military)


[5. Explainable artificial intelligence: a comprehensive review](https://link.springer.com/article/10.1007/s10462-021-10088-y)

### Aim
    Providing a comprehensive overview of the latest progress in explainable artificial intelligence.

### Main focuses
    Giving an introduction and motivation
    Talking about the background and concepts
    Categorization of XAI Approaches
    Going through each category and explaining them

### Methodology
     Providing a theoretical foundation of XAI,
    Categorizing the latest XAI studies into pre-modeling explainability, interpretable mode, and post-modeling explainability
    Discussing and comparing the advantages and drawbacks of each approach
    Analyzing the  research that equips explainability to the deep learning models
    Discussing various challenges and show the future research ideas   

### Applications
    Healthcare and Medicine
    Autonomous Systems
    Finance
    Retail and E-commerce
    Natural Language Processing


[6. Metrics for Explainable AI: Challenges and Prospects](https://arxiv.org/ftp/arxiv/papers/1812/1812.04608.pdf)

### Aim
    Focusing on the key concepts of measurement whether an XAI is really give a good explaination

### Main focuses
    Focusing on,
    The Goodness of explanations
    Whether users are satisfied by explanations
    How well users understand the AI system
    How curiosity motivates the search for explanations
    Whether the user's trust and reliance on the AI are appropriate
    How the human-XAI work system performs
    
### Methodology
    Looking at existing studies, literature, and tools that measure things like curiosity and trust in AI systems.
    Developing new ways to measure curiosity and trust that make sense for AI systems that explain their decisions
    Using a variety of methods to study human interactions with AI

### Results
    Introduces the concepts of explanation goodness and explanation satisfaction, categorizes different types of explanations, and proposes 
    measurement scales to assess the quality of explanations provided by XAI systems
    
### Applications
    Healthcare
    Education
    Energy and Environment
    Manufacturing and Industry
    Human ResourcesCriminal Justice
    Customer Service
    Legal and Compliance

### Limitations
    The proposed measurement scales and methodologies might assume that they can be applied universally across different XAI systems and application             
    domains.However, the effectiveness and suitability of these measures could vary based on the specific context
    The article acknowledges that its ideas and methods are not final but represent a foundation for further exploration.


[7. Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/pdf/1702.08608.pdf)

### Aim
    Advancing the understanding of interpretability in AI and provide a structured framework for evaluating and improving the interpretability of AI systems

### Main focuses
    To define, evaluate, and advance the concept of interpretability in artificial intelligence and machine learning, addressing challenges, 
    identifying factors, and providing guidance to researchers to make AI systems more transparent and understandable to humans.
    
### Methodology
    Defining and conceptualizing the term "interpretability"
    Proposing an evaluation framework for interpretability
    Identifying and discussing the challenges and open problems
    Exploring factors and latent dimensions that may impact interpretability
    Emphasizing the importance of bridging the gap between theoretical concepts and practical applicati+J15:K15ons of interpretability
    
### Results
    Outlines the objectives, focus areas, and methodology for addressing the topic of interpretability
    
### Applications
    Real-World AI systems
    When ensuring safety and reliability
    When concerning about the ethical considerations


[8. Explanation and Justification in Machine Learning: A Survey ](http://www.cs.columbia.edu/~orb/papers/xai_survey_paper_2017.pdf)

### Aim
    Providing a comprehensive overview of the historical background, current developments, and challenges in the field of explanation and 
    justification in machine learning (ML).

### Main focuses
    Providing Historical and Current Developments
    Giving Explanation in Different Domains
    Provide Approaches to Explanation
    Provide Challenges and Future Directions
    To give a Comparative Analysis on Explanation

### Methodology
    Categorizing information from diverse sources, providing comparative analyses of explanation methods, integrating interdisciplinary 
    insights, and identifying challenges and future directions to comprehensively overview the field of explanation and justification 
    in machine learning.
    
### Results
    Providing an Comprehensive Overview
    Giving a proper Categorization and Comparative Analysis
    Identification of Challenges

### Applications
    Interpretable AI Systems
    Decision Support Systems
    Ethical AI Development
    Legal and Compliance Purposes

### Limitations
    Machine learning and AI are rapidly evolving fields. Information presented has become a bit outdated 


## Journal Articles

[1. Randomized Input Sampling forExplanation of Black-box Models](https://arxiv.org/pdf/1806.07421.pdf)

### Aim
    Addressing the problem of Explainable AI (XAI) for deep neural networks, particularly those that process images and make classification decisions.

### Main focuses
    Generating effective explanations through importance maps and assessing their quality using novel causal metrics and human-centric evaluation methods.
    
### Methodology
    Methodology revolves around the development of an approach called "RISE" (Randomized Input Sampling for Explanation) for explaining the decision-making           process of deep neural networks, particularly when they are treated as black-box models. RISE aims to generate importance maps for input images to help           explain the model's predictions. 
    
### Results
     Proposed RISE approach performs well in explaining the decisions made by deep neural networks. 
     The results are evaluated on various benchmark datasets,
         Deletion and Insertion Metrics
         Pointing Game Accuracy
         Versatility and Black-Box Capability
         Comparative Evaluation
         Robustness and Variability
         Application to Image Captioning
    
### Applications
    Model Debugging
    Medical Diagnosis
    Autonomous Systems
    Criminal Justice
    Content Recommendation
    Image Captioning
    
### Limitations
     A larger number of samples may provide more accurate explanations but can be computationally expensive.
     RISE explanations can sometimes be noisy or inconsistent
     Difficult to handle large images
     Limited to Image Inputs

[2. Anchors: High-Precision Model-Agnostic Explanations](https://ojs.aaai.org/index.php/aaai/article/view/11491)

### Aim
    Addressing the challenge of providing interpretable explanations for the local behavior of complex machine learning models.
    
### Main focuses
    To introduce a new method for explaining the behavior of complex machine learning models and it should be Model-agnostic, with 
    High-precision and Interpretable.
    
### Methodology
    Introducing an Anchor Generation Algorithm(anchors- the conditions (rules) under which a model's prediction remains stable or consistent) 
    with a Model-Agnostic Approach having High-Precision Explanations. 
    
### Results
     Anchor explanations consistently delivered high precision, outperforming LIME explanations, with clear coverage, user preference, and 
     greater efficiency, as confirmed in a user study, validating their effectiveness in enhancing the interpretability of machine learning models.
    
### Applications
    Can be used to enhance model interpretability in fields like healthcare, AI systems, regulatory compliance, user interfaces, algorithmic 
    fairness, education, medical diagnosis, content recommendation, image and text classification, and automated decision systems.
    
### Limitations
     Overly Specific Anchors
     Potential Anchor Conflicts
     Challenges in Complex Output Spaces
     Designing Realistic Perturbation Distributions
     
[3. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI](https://www.sciencedirect.com/science/article/pii/S1566253519308103?casa_token=4neUzV9O7lUAAAAA:dQjGSGDeMTjfQfSqRpa2dI8wiqN4iBrvcuwISBhQ3ojN4m9TheforJM_I-mLs017WOEDSptyAg85)

### Aim
    Creatinge a comprehensive XAI taxonomy for newcomers, inspiring future research while encouraging professionals from diverse fields 
    to embrace AI's benefits regardless of its interpretability.
    
### Main focuses
    Understanding XAI
    Identifying taxonomies of XAI
    Challenges of XAI
    Exppress about esponsible AI
    How Privacy and Security can be addressed using XAI
    
### Methodology
    Carrying out a  systematic literature review, taxonomies development, and critical analysis of the challenges and implications of XAI.
    
### Applications
    Academic Research Purposes
    For Industry and Practitioners
    For Policy Makers and Regulators
    For AI Developers
    Data Privacy and Security
    Educational Purposes

### Limitations
    The study may not cover every single contribution or development in the field of XAI
    The categorization and taxonomy development in the study may involve some degree of subjectivity.
     

[4. Explanation in artificial intelligence: Insights from the social sciences](https://www.sciencedirect.com/science/article/pii/S0004370218305988)

### Aim
    Promoting the infusion of social science research into the field of explainable AI
    
### Main focuses
    Exploring Intersection of XAI and Social Sciences
    Understanding Human Explanation Process
    Motivating Example and Interactive Dialogue(The athropode question)
    Acknowledging challenges in integrating social science insights into AI models
    Enhancing Trust and Transparency in XAI models
    
### Methodology
    Discussing the theoretical framework and key concepts related to explainable artificial intelligence (XAI) and how social science 
    insights can be integrated into the field.
    Then synthesizing existing knowledge and proposing ideas for integrating social science insights into the development of AI systems
    
### Results
    Proving that Explainable AI can benefit from existing models of how people define, generate, select, present, and 
    evaluate explanations.
    Giving 4 Key points to consider when creating XAI,
    (1) why-questions are contrastive
    (2) explanations are selected (in a biased manner)
    (3) explanations are social
    (4)probabilities are not as important as causal links 
    
### Applications
    The article discusses the need for explanations in,
    Justifying Autonomous Agent Behavior
    Debugging Machine Learning Models
    Explaining Medical Decision-Making
    Explaining Predictions of Classifiers
    
### Limitations
    Adopting the studie's work into explainable AI is not a straightforward step.
    (From a social science viewpoint, these models will need to be refined and extended to provide good explanatory agents, 
    which requires researchers in explainable AI to work closely with researchers from philosophy, psychology, cognitive science, 
    and human–computer interaction)


